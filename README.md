# Better DEV AI Backend

> Core server and API for Better DEV, an intelligent AI chat platform with tool-calling capabilities and streaming responses. 

[![NestJS](https://img.shields.io/badge/NestJS-E0234E?style=flat&logo=nestjs&logoColor=white)](https://nestjs.com/)
[![TypeScript](https://img.shields.io/badge/TypeScript-3178C6?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-336791?style=flat&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Prerequisites](#-prerequisites)
- [Getting Started](#-getting-started)
- [Project Structure](#-project-structure)
- [API Documentation](#-api-documentation)
- [Operational Modes](#-operational-modes)
- [Tool System](#-tool-system)
- [Deployment](#-deployment)
- [Development](#-development)
- [Environment Variables](#-environment-variables)
- [Contributing](#-contributing)

---

## ğŸŒŸ Overview

Better DEV AI Backend is a production-ready NestJS application that powers an intelligent conversational AI platform. It provides:

- **Real-time AI Conversations** with streaming responses using AI SDK v5
- **Intelligent Tool Calling** system with web search capabilities
- **Conversation Management** with persistent storage
- **User Authentication** with JWT-based security
- **Multi-Model AI Support** using Groq (Llama models)
- **Extensible Tool Architecture** for adding custom capabilities

---

## ğŸ—ï¸ Architecture

### High-Level Design (HLD)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Client Layer                            â”‚
â”‚  (React/Next.js Frontend, Mobile Apps, API Clients)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTPS/REST
                     â”‚ Server-Sent Events (SSE)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API Gateway (Nginx)                        â”‚
â”‚  - Load Balancing                                               â”‚
â”‚  - SSL Termination                                              â”‚
â”‚  - Request Routing                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   NestJS Application Layer                      â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Controllers (REST Endpoints)               â”‚    â”‚
â”‚  â”‚  - AuthController: /auth/*                              â”‚    â”‚
â”‚  â”‚  - ChatController: /chat/*                              â”‚    â”‚
â”‚  â”‚  - HealthController: /health                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â”‚                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                  Service Layer                          â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚ AuthService â”‚  â”‚ ChatService  â”‚  â”‚  UserService  â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â”‚         â”‚                â”‚                  â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚    AIService           â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚  - analyzeQueryIntent()â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚  - streamResponseWith  â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚    Mode()              â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚  - generateResponse()  â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚                â”‚                  â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ Operational Modes      â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚                        â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â”‚ ModeResolver       â”‚ â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â”‚ Service            â”‚ â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚           â”‚            â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â”‚ AutoClassifier     â”‚ â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â”‚ Service            â”‚ â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚           â”‚            â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â”‚ ClassificationCacheâ”‚ â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â”‚ Service            â”‚ â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚           â”‚    â”‚
â”‚  â”‚         â”‚                                   â”‚           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚            â”‚                                   â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Tool System (Extensible)                    â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚         ToolRegistry (Service Locator)           â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - register()  - get()  - toAISDKFormat()        â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                       â”‚                                  â”‚   â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚  â”‚         â”‚                           â”‚                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚  â”‚ WebSearchTool â”‚      â”‚  Future Tools    â”‚             â”‚   â”‚
â”‚  â”‚  â”‚ - execute()   â”‚      â”‚  - Calculator    â”‚             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  - Weather       â”‚             â”‚   â”‚
â”‚  â”‚          â”‚              â”‚  - ImageGen      â”‚             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â”‚  â”‚ TavilyService    â”‚                                    â”‚   â”‚
â”‚  â”‚  â”‚ SummaryService   â”‚                                    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚            Data Access Layer (TypeORM)                  â”‚    â”‚
â”‚  â”‚  - Repositories: User, Conversation, Message            â”‚    â”‚
â”‚  â”‚  - Entity Models with Relations                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PostgreSQL Database                            â”‚
â”‚                                                                 â”‚
â”‚  Tables:                                                        â”‚
â”‚  - users (id, email, password, credits, isActive)               â”‚
â”‚  - conversations (id, userId, title, systemPrompt,              â”‚
â”‚                   operationalMode)                              â”‚
â”‚  - messages (id, conversationId, role, content, metadata)       â”‚
â”‚                                                                 â”‚
â”‚  Relationships:                                                 â”‚
â”‚  - User 1:N Conversations                                       â”‚
â”‚  - Conversation 1:N Messages                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   External Services                             â”‚
â”‚  - Groq API (AI Models: Llama 3.1, Llama 3.3)                   â”‚
â”‚  - Tavily API (Web Search)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interactions

```
User Request Flow:
1. Client â†’ Nginx â†’ NestJS Controller
2. Controller validates JWT via JwtAuthGuard
3. Controller â†’ Service Layer
4. Service Layer â†’ AI Service (with Tool Registry)
5. AI Service â†’ Groq API (streaming)
6. If tool needed â†’ Tool Registry â†’ Specific Tool â†’ External API
7. Stream results back through SSE â†’ Client

Database Transaction Flow:
1. Service Layer â†’ TypeORM Repository
2. Repository â†’ PostgreSQL
3. Response â†’ Service â†’ Controller â†’ Client
```

---

## âœ¨ Features

### Core Capabilities

- **ğŸ¤– Multi-Model AI Support**
  - Fast text generation with Llama 3.1 8B
  - Advanced tool calling with Llama 3.3 70B
  - **Native Vision support** with Llama 4 Scout (Automatic detection)
  - Automatic model selection based on task and content type

- **ğŸ”§ Extensible Tool System**
  - Plugin-based architecture
  - Type-safe tool definitions with Zod
  - Automatic tool registration
  - Built-in web search with Tavily
  - Automatically determines if queries need web search

- **ğŸ’¬ Conversation Management**
  - Persistent history with JSONB parts support
  - **Subquery Batching**: sidebar loads instantly by fetching only the latest message preview.
  - **O(1) Enrichment**: Map-based lookup for instant attachment injection.
  - Vision History Window: Automatically manages model limits (max 5 images).
  - **Index-Safe Restoration**: Ensures zero data loss during SDK message transformation.

- **ğŸ” Security**
  - JWT-based authentication
  - Password hashing with bcrypt
  - Request validation with class-validator
  - CORS configuration

- **ğŸ¯ Operational Modes** (NEW)
  - **Fast Mode** - Quick responses with Llama 3.1 8B (500 tokens, 0.5 temp)
  - **Thinking Mode** - Deep analysis with Llama 3.3 70B (4000 tokens, 0.7 temp)
  - **Auto Mode** - AI-powered classification (default)
  - Intelligent query complexity detection
  - 5-minute classification caching (70% API call reduction)
  - Per-conversation and per-message mode control
  - Mode metadata tracking in message history

- **ğŸ“¡ Real-time Streaming**
  - Server-Sent Events (SSE)
  - AI SDK v5 compatible
  - Tool execution visibility
  - Progress tracking

---

## ğŸ› ï¸ Tech Stack

### Backend Framework
- **NestJS** - Progressive Node.js framework
- **TypeScript** - Type-safe development
- **Express** - HTTP server

### Database
- **PostgreSQL** - Relational database
- **TypeORM** - ORM with entity management

### AI & Tools
- **AI SDK v5** (Vercel) - Unified AI interface
- **Groq** - Fast AI inference
- **Tavily** - Web search API

### Authentication
- **Passport JWT** - Token-based auth
- **bcrypt** - Password hashing

### Validation
- **class-validator** - DTO validation
- **class-transformer** - Object transformation
- **Zod** - Schema validation for tools

### DevOps
- **Docker** - Containerization
- **Docker Compose** - Multi-container orchestration
- **Nginx** - Reverse proxy & load balancing

---

## ğŸ“¦ Prerequisites

- **Node.js** 20+ (LTS recommended)
- **npm** or **yarn**
- **PostgreSQL** 15+
- **Docker** & **Docker Compose** (for containerized setup)
- **Groq API Key** ([Get one here](https://console.groq.com))
- **Tavily API Key** ([Get one here](https://tavily.com))

---

## ğŸš€ Getting Started

### Option 1: Docker (Recommended)

1. **Clone the repository**
   ```bash
   git clone https://github.com/Kashif-Rezwi/better-dev-api.git
   cd better-dev-api
   ```

2. **Create environment file**
   ```bash
   cp .env.docker .env
   ```

3. **Configure environment variables**
   ```bash
   # Edit .env file
   JWT_SECRET=your-super-secret-jwt-key-change-this
   GROQ_API_KEY=gsk_your_groq_api_key
   TAVILY_API_KEY=tvly-your-tavily-api-key
   DEFAULT_AI_MODEL=openai/gpt-oss-120b
   AI_TEXT_MODEL=llama-3.1-8b-instant
   AI_TOOL_MODEL=llama-3.3-70b-versatile
   ```

4. **Start the application**
   ```bash
   npm run docker:up
   ```

5. **Check health**
   ```bash
   curl http://localhost:3001/health
   ```

6. **View logs**
   ```bash
   npm run docker:logs
   ```

### Option 2: Local Development

1. **Clone the repository**
   ```bash
   git clone https://github.com/Kashif-Rezwi/better-dev-api.git
   cd better-dev-api
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Setup PostgreSQL**
   ```bash
   # Install PostgreSQL locally or use Docker
   docker run --name nebula-postgres \
     -e POSTGRES_USER=nebula_ai \
     -e POSTGRES_PASSWORD=nebula_ai \
     -e POSTGRES_DB=nebula_db \
     -p 5432:5432 \
     -d postgres:15-alpine
   ```

4. **Create .env file**
   ```bash
   # Database
   DATABASE_HOST=localhost
   DATABASE_PORT=5432
   DATABASE_USER=nebula_ai
   DATABASE_PASSWORD=nebula_ai
   DATABASE_NAME=nebula_db

   # JWT
   JWT_SECRET=your-super-secret-jwt-key
   JWT_EXPIRATION=7d

   # App
   PORT=3001
   NODE_ENV=development

   # AI
   GROQ_API_KEY=gsk_your_groq_api_key
   DEFAULT_AI_MODEL=openai/gpt-oss-120b
   AI_TEXT_MODEL=llama-3.1-8b-instant
   AI_TOOL_MODEL=llama-3.3-70b-versatile

   # Tools
   TAVILY_API_KEY=tvly-your-tavily-api-key
   ```

5. **Start development server**
   ```bash
   npm run start:dev
   ```

6. **Access the API**
   - API: http://localhost:3001
   - Health: http://localhost:3001/health

---

## ğŸ“ Project Structure

```
better-dev-api/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/                      # Configuration files
â”‚   â”‚   â”œâ”€â”€ database.config.ts       # Database connection config
â”‚   â”‚   â””â”€â”€ jwt.config.ts            # JWT authentication config
â”‚   â”‚
â”‚   â”œâ”€â”€ common/                      # Shared resources
â”‚   â”‚   â””â”€â”€ guards/
â”‚   â”‚       â””â”€â”€ jwt-auth.guard.ts    # JWT authentication guard
â”‚   â”‚
â”‚   â”œâ”€â”€ modules/                     # Feature modules
â”‚   â”‚   â”œâ”€â”€ auth/                    # Authentication module
â”‚   â”‚   â”‚   â”œâ”€â”€ dto/                 # Data Transfer Objects
â”‚   â”‚   â”‚   â”œâ”€â”€ strategies/          # Passport strategies
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.controller.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.service.ts
â”‚   â”‚   â”‚   â””â”€â”€ auth.module.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ user/                    # User management module
â”‚   â”‚   â”‚   â”œâ”€â”€ entities/            # Database entities
â”‚   â”‚   â”‚   â”œâ”€â”€ dto/
â”‚   â”‚   â”‚   â”œâ”€â”€ user.service.ts
â”‚   â”‚   â”‚   â””â”€â”€ user.module.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ chat/                    # Chat & AI module
â”‚   â”‚       â”œâ”€â”€ entities/            # Conversation, Message
â”‚   â”‚       â”œâ”€â”€ dto/
â”‚   â”‚       â”œâ”€â”€ tools/               # Tool system
â”‚   â”‚       â”‚   â”œâ”€â”€ implementations/ # Tool implementations
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ web-search.tool.ts
â”‚   â”‚       â”‚   â”œâ”€â”€ interfaces/      # Tool interfaces
â”‚   â”‚       â”‚   â”œâ”€â”€ services/        # Tool services
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ tavily.service.ts
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ summary.service.ts
â”‚   â”‚       â”‚   â”œâ”€â”€ tool.registry.ts # Tool registry
â”‚   â”‚       â”‚   â””â”€â”€ tools.config.ts  # Tool configuration
â”‚   â”‚       â”œâ”€â”€ chat.controller.ts
â”‚   â”‚       â”œâ”€â”€ chat.service.ts
â”‚   â”‚       â”œâ”€â”€ ai.service.ts
â”‚   â”‚       â””â”€â”€ chat.module.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ app.module.ts                # Root module
â”‚   â”œâ”€â”€ main.ts                      # Application entry point
â”‚   â””â”€â”€ health.controller.ts         # Health check endpoint
â”‚
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ nginx.conf                   # Nginx configuration
â”‚
â”œâ”€â”€ docker-compose.yml               # Docker services
â”œâ”€â”€ Dockerfile                       # Production Docker image
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

---

## ğŸ“¡ API Documentation

### Base URL
- Development: `http://localhost:3001`
- Production: Configure via Nginx

### Authentication

#### Register User
```http
POST /auth/register
Content-Type: application/json

{
  "email": "user@example.com",
  "password": "securepassword123"
}

Response: 201 Created
{
  "accessToken": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "user": {
    "id": "uuid",
    "email": "user@example.com",
    "credits": 1000
  }
}
```

#### Login
```http
POST /auth/login
Content-Type: application/json

{
  "email": "user@example.com",
  "password": "securepassword123"
}

Response: 200 OK
{
  "accessToken": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "user": {
    "id": "uuid",
    "email": "user@example.com",
    "credits": 1000
  }
}
```

#### Get Profile
```http
GET /auth/profile
Authorization: Bearer {token}

Response: 200 OK
{
  "userId": "uuid",
  "email": "user@example.com"
}
```

### Conversations

#### Create Conversation with First Message
```http
POST /chat/conversations/with-message
Authorization: Bearer {token}
Content-Type: application/json

{
  "title": "New Conversation",
  "systemPrompt": "You are a helpful assistant.",
  "firstMessage": "Hello, how are you?"
}

Response: 201 Created
{
  "id": "conv-uuid",
  "title": "New Conversation",
  "systemPrompt": "You are a helpful assistant.",
  "createdAt": "2025-01-01T00:00:00.000Z",
  "updatedAt": "2025-01-01T00:00:00.000Z"
}
```

#### List Conversations
```http
GET /chat/conversations
Authorization: Bearer {token}

Response: 200 OK
[
  {
    "id": "uuid",
    "title": "Conversation Title",
    "systemPrompt": "Custom prompt",
    "createdAt": "2025-01-01T00:00:00.000Z",
    "updatedAt": "2025-01-01T00:00:00.000Z",
    "lastMessage": {
      "id": "msg-uuid",
      "role": "assistant",
      "content": "Last message preview...",
      "createdAt": "2025-01-01T00:00:00.000Z"
    }
  }
]
```

#### Get Conversation
```http
GET /chat/conversations/{id}
Authorization: Bearer {token}

Response: 200 OK
{
  "id": "uuid",
  "title": "Conversation Title",
  "systemPrompt": "Custom prompt",
  "createdAt": "2025-01-01T00:00:00.000Z",
  "updatedAt": "2025-01-01T00:00:00.000Z",
  "messages": [
    {
      "id": "msg-uuid",
      "role": "user",
      "content": "Hello",
      "createdAt": "2025-01-01T00:00:00.000Z"
    },
    {
      "id": "msg-uuid-2",
      "role": "assistant",
      "content": "Hi there!",
      "createdAt": "2025-01-01T00:00:01.000Z",
      "metadata": {
        "toolCalls": []
      }
    }
  ]
}
```

#### Send Message (Streaming)
```http
POST /chat/conversations/{id}/messages
Authorization: Bearer {token}
Content-Type: application/json

{
  "messages": [
    {
      "id": "msg-1",
      "role": "user",
      "parts": [
        {
          "type": "text",
          "text": "What's the latest news about AI?"
        }
      ]
    }
  ]
}

Response: 200 OK (Server-Sent Events)
Content-Type: text/event-stream

data: {"type":"text-delta","textDelta":"The latest"}
data: {"type":"text-delta","textDelta":" news..."}
data: {"type":"tool-call-start","toolName":"tavily_web_search"}
data: {"type":"tool-result","result":{...}}
data: {"type":"finish","usage":{...}}
```

#### Update System Prompt
```http
PUT /chat/conversations/{id}/system-prompt
Authorization: Bearer {token}
Content-Type: application/json

{
  "systemPrompt": "You are an expert in AI and machine learning."
}

Response: 200 OK
{
  "id": "uuid",
  "title": "Conversation Title",
  "systemPrompt": "You are an expert in AI and machine learning.",
  "createdAt": "2025-01-01T00:00:00.000Z",
  "updatedAt": "2025-01-01T00:00:00.000Z"
}
```

#### Generate Title
```http
POST /chat/conversations/{id}/generate-title
Authorization: Bearer {token}
Content-Type: application/json

{
  "message": "What's the weather like today?"
}

Response: 200 OK
{
  "title": "Weather Inquiry"
}
```

#### Delete Conversation
```http
DELETE /chat/conversations/{id}
Authorization: Bearer {token}

Response: 204 No Content
```

### Health Check

```http
GET /health

Response: 200 OK
{
  "status": "ok",
  "timestamp": "2025-01-01T00:00:00.000Z",
  "service": "better-dev-ai-chat"
}
```

---

## ğŸ§  AI Service & Intelligent Query Routing

### Query Intent Analysis

The AI Service includes an intelligent query intent analyzer that optimizes tool usage and reduces unnecessary API calls. The `analyzeQueryIntent()` method automatically determines whether a user query requires real-time web search or can be answered from general knowledge.

### How It Works

```typescript
// Automatically analyzes each query
const needsWebSearch = await aiService.analyzeQueryIntent(messages);

// Routes to appropriate model
if (needsWebSearch) {
  // Use tool-calling model with web search
  stream = aiService.streamResponse(messages, tools);
} else {
  // Use fast text model without tools
  stream = aiService.streamResponse(messages);
}
```

### Intelligence Features

**âœ… Queries that trigger web search:**
- Current/recent events and news (e.g., "latest AI trends 2025")
- Real-time information (e.g., "today's weather", "current stock price")
- Up-to-date data that changes frequently

**âŒ Queries that use general knowledge:**
- General knowledge questions (e.g., "What is JavaScript?", "Explain OOP")
- Follow-up questions to previous searches (context already available)
- Questions about AI capabilities
- General conversation and clarification

### Smart Conversation Context

The analyzer checks recent conversation history (last 3 turns) to detect if web search was recently performed. This prevents redundant searches when:
- User asks follow-up questions about search results
- Context is already available from previous tool calls
- User is refining or clarifying a previous query

### Benefits

- **ğŸ’° Cost Optimization** - Avoids unnecessary tool-calling model usage
- **âš¡ Faster Responses** - Uses lightweight models when appropriate
- **ğŸ¯ Better UX** - Reduces wait time for simple queries
- **ğŸ” Smarter Tool Usage** - Only searches when truly needed

### Configuration

The feature uses different AI models for different tasks:

```env
# Fast model for query analysis and simple responses
AI_TEXT_MODEL=llama-3.1-8b-instant

# Powerful model for tool calling and complex tasks
AI_TOOL_MODEL=llama-3.3-70b-versatile
```

### Example Flow

```
User: "What is machine learning?"
  â†“
AnalyzeQueryIntent() â†’ NO (general knowledge)
  â†“
Use fast text model â†’ Quick response
  â†“
User: "What are the latest ML breakthroughs in 2025?"
  â†“
AnalyzeQueryIntent() â†’ YES (current events)
  â†“
Use tool model + web search â†’ Real-time results
```

---

## ğŸ¯ Operational Modes

### Overview

The Operational Modes system provides intelligent chat response modes that automatically adjust AI model selection, token limits, and response styles based on query complexity and user preferences.

**Available Modes:**
- **Fast Mode** - Quick, concise responses using lightweight models (Llama 3.1 8B)
- **Thinking Mode** - Detailed, comprehensive responses using advanced models (Llama 3.3 70B)
- **Auto Mode** - AI-powered automatic mode selection based on query complexity (default)

---

### High-Level Design (HLD)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Request                            â”‚
â”‚  POST /chat/conversations/:id/messages                          â”‚
â”‚  { messages, modeOverride?: "fast"|"thinking"|"auto" }          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ChatController                               â”‚
â”‚  - Validates request                                            â”‚
â”‚  - Extracts modeOverride (optional)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ChatService                                  â”‚
â”‚  - handleStreamingResponse()                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  â”‚      MODE RESOLUTION HIERARCHY             â”‚
               â”‚  â”‚                                            â”‚
               â”‚  â”‚  Priority (highest to lowest):             â”‚
               â”‚  â”‚  1. Message-level override (request)       â”‚
               â”‚  â”‚  2. Conversation-level setting (DB)        â”‚
               â”‚  â”‚  3. Default mode ("auto")                  â”‚
               â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ModeResolverService                           â”‚
â”‚  - resolveMode()                                                 â”‚
â”‚  - getRequestedMode() â†’ Returns: "fast"|"thinking"|"auto"        â”‚
â”‚  - resolveEffectiveMode() â†’ Returns: "fast"|"thinking"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ If mode === "auto"
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 AutoClassifierService                            â”‚
â”‚  Uses AI to classify query complexity                            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           Classification Logic                           â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  1. Check ClassificationCacheService (5-min TTL)         â”‚    â”‚
â”‚  â”‚  2. Quick heuristic: queries < 15 chars â†’ fast           â”‚    â”‚
â”‚  â”‚  3. AI classification prompt                             â”‚    â”‚
â”‚  â”‚     - SIMPLE queries â†’ fast mode                         â”‚    â”‚
â”‚  â”‚     - COMPLEX queries â†’ thinking mode                    â”‚    â”‚
â”‚  â”‚  4. Cache result for future requests                     â”‚    â”‚
â”‚  â”‚  5. Timeout fallback: 5s â†’ defaults to fast              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Returns: { requested, effective }
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODE_CONFIG                                   â”‚
â”‚  Static configuration for each mode                              â”‚
â”‚                                                                  â”‚
â”‚  fast: {                         thinking: {                     â”‚
â”‚    model: llama-3.1-8b-instant     model: llama-3.3-70b-versatileâ”‚
â”‚    maxTokens: 500                  maxTokens: 4000               â”‚
â”‚    temperature: 0.5                temperature: 0.7              â”‚
â”‚    systemPrompt: "Be concise"      systemPrompt: "Be detailed"   â”‚
â”‚  }                                }                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Pass effective mode config
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIService                                     â”‚
â”‚  - streamResponseWithMode(messages, effectiveMode, ...)          â”‚
â”‚  - Applies mode config (model, tokens, temperature, prompt)      â”‚
â”‚  - Streams response with configured parameters                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Stream response + save metadata
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Message Saved with Metadata                       â”‚
â”‚  {                                                               â”‚
â”‚    content: "...",                                               â”‚
â”‚    metadata: {                                                   â”‚
â”‚      mode: {                                                     â”‚
â”‚        requested: "auto",                                        â”‚
â”‚        effective: "thinking",                                    â”‚
â”‚        modelUsed: "llama-3.3-70b-versatile",                     â”‚
â”‚        temperature: 0.7                                          â”‚
â”‚      }                                                           â”‚
â”‚    }                                                             â”‚
â”‚  }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Low-Level Design (LLD)

#### Component Breakdown

**1. Mode Types & Configuration (`mode.config.ts`)**

```typescript
// Type definitions
export type OperationalMode = 'fast' | 'thinking' | 'auto';
export type EffectiveMode = 'fast' | 'thinking';

// Configuration per mode
export const MODE_CONFIG: Record<EffectiveMode, ModeConfig> = {
  fast: {
    model: 'llama-3.1-8b-instant',
    maxTokens: 500,
    temperature: 0.5,
    systemPrompt: 'Be extremely concise and direct...'
  },
  thinking: {
    model: 'llama-3.3-70b-versatile',
    maxTokens: 4000,
    temperature: 0.7,
    systemPrompt: 'Provide thorough, comprehensive responses...'
  }
};
```

**2. ModeResolverService** (`mode-resolver.service.ts`)

Resolves operational mode using hierarchy:

```
Request Priority:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 1. modeOverride (API request body)      â”‚ â† Highest priority
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 2. conversation.operationalMode (DB)    â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 3. Default: "auto"                      â”‚ â† Lowest priority
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Methods:**
- `resolveMode()` - Main entry point
- `getRequestedMode()` - Determines mode via hierarchy
- `resolveEffectiveMode()` - Converts "auto" to concrete mode

**3. AutoClassifierService** (`auto-classifier.service.ts`)

AI-powered query complexity analysis:

```
Classification Flow:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 1. Extract last user message            â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 2. Quick heuristic (< 15 chars â†’ fast)  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 3. Check ClassificationCacheService     â”‚
  â”‚    - Cache hit â†’ return cached mode     â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 4. AI classification (with 5s timeout)  â”‚
  â”‚    - Send prompt to lightweight model   â”‚
  â”‚    - "SIMPLE" â†’ fast mode               â”‚
  â”‚    - "COMPLEX" â†’ thinking mode          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 5. Cache result (5-minute TTL)          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 6. Return effective mode                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Classification Criteria:**

| Query Type | Mode | Examples |
|------------|------|----------|
| Short questions | Fast | "What is X?", "Define Y" |
| Factual lookups | Fast | "Who invented Z?" |
| Yes/no questions | Fast | "Can I do X?" |
| Multi-part analysis | Thinking | "Compare X and Y in detail" |
| Code implementation | Thinking | "Build a function that..." |
| Debugging requests | Thinking | "Why is this code failing?" |
| Design decisions | Thinking | "Design a system for..." |

**4. ClassificationCacheService** (`classification-cache.service.ts`)

In-memory caching to reduce AI calls:

```
Cache Behavior:
  - TTL: 5 minutes per entry
  - Key: MD5 hash of last user message text
  - Cleanup: Every 60 seconds (removes expired)
  - Storage: Map<string, CacheEntry>
```

**Benefits:**
- Reduces classification API calls by ~70%
- Improves response time for repeated queries
- Automatic expiration ensures fresh classifications

---

### Database Schema

**Conversation Entity** (`conversation.entity.ts`)

```sql
ALTER TABLE conversations ADD COLUMN operational_mode VARCHAR(20);
-- Values: 'fast' | 'thinking' | 'auto' | NULL

-- NULL means use default behavior (auto mode)
```

**Message Metadata** (`message.entity.ts`)

```typescript
// Messages store mode metadata in JSONB
{
  "metadata": {
    "toolCalls": [...],  // Existing tool call data
    "mode": {            // NEW: Mode tracking
      "requested": "auto",
      "effective": "thinking",
      "modelUsed": "llama-3.3-70b-versatile",
      "temperature": 0.7,
      "tokensUsed": null  // Future: token tracking
    }
  }
}
```

---

### API Endpoints

#### **1. Send Message with Mode Override**

```http
POST /chat/conversations/:id/messages
Authorization: Bearer {token}
Content-Type: application/json

{
  "messages": [
    {
      "role": "user",
      "parts": [{ "type": "text", "text": "Explain quantum computing" }]
    }
  ],
  "modeOverride": "thinking"  // Optional: "fast" | "thinking" | "auto"
}

Response: 200 OK (Server-Sent Events)
```

#### **2. Update Conversation Mode**

```http
PUT /chat/conversations/:id/operational-mode
Authorization: Bearer {token}
Content-Type: application/json

{
  "mode": "fast"  // "fast" | "thinking" | "auto"
}

Response: 200 OK
{
  "id": "conv-uuid",
  "title": "Conversation Title",
  "systemPrompt": "...",
  "operationalMode": "fast",  // Updated mode
  "createdAt": "2025-01-01T00:00:00.000Z",
  "updatedAt": "2025-01-01T00:00:00.000Z"
}
```

#### **3. Create Conversation with Mode**

```http
POST /chat/conversations/with-message
Authorization: Bearer {token}
Content-Type: application/json

{
  "title": "New Chat",
  "systemPrompt": "You are a helpful assistant.",
  "operationalMode": "thinking",  // Optional
  "firstMessage": "Hello!"
}

Response: 201 Created
{
  "id": "conv-uuid",
  "title": "New Chat",
  "operationalMode": "thinking",
  ...
}
```

---

### Request Flow Example

**Scenario:** User sends "Explain how databases work internally" with no mode override

```
1. Request arrives at ChatController
   â””â”€> No modeOverride in request

2. ChatService.handleStreamingResponse()
   â””â”€> Calls ModeResolverService.resolveMode()

3. ModeResolverService.getRequestedMode()
   â”œâ”€> Check modeOverride: null
   â”œâ”€> Check conversation.operationalMode: null
   â””â”€> Default: "auto"
   â””â”€> Requested mode = "auto"

4. ModeResolverService.resolveEffectiveMode()
   â””â”€> Mode is "auto" â†’ delegate to AutoClassifierService

5. AutoClassifierService.classify()
   â”œâ”€> Extract query: "Explain how databases work internally"
   â”œâ”€> Length check: 38 chars (> 15) â†’ continue
   â”œâ”€> Cache check: MISS (not cached)
   â”œâ”€> AI classification prompt sent:
   â”‚   "Query: 'Explain how databases work internally'"
   â”‚   Response: "COMPLEX"
   â”œâ”€> Result: "thinking" mode
   â””â”€> Cache result with key: md5(query)

6. Return to ChatService
   â””â”€> { requested: "auto", effective: "thinking" }

7. ChatService gets MODE_CONFIG["thinking"]
   â””â”€> model: llama-3.3-70b-versatile
   â””â”€> maxTokens: 4000
   â””â”€> temperature: 0.7
   â””â”€> systemPrompt: "Provide thorough, comprehensive..."

8. AIService.streamResponseWithMode()
   â””â”€> Apply mode config to streaming request

9. Response streamed to client
   â””â”€> Save message with mode metadata

10. Next identical query
    â””â”€> Cache HIT â†’ skip AI classification
    â””â”€> Instant mode resolution (< 1ms)
```

---

### Performance Characteristics

| Operation | Latency | Notes |
|-----------|---------|-------|
| Mode resolution (no auto) | < 1ms | Direct lookup |
| Cache hit (auto mode) | < 1ms | MD5 hash lookup |
| Cache miss (auto mode) | ~100-500ms | AI classification call |
| Classification timeout | 5s | Fallback to fast mode |

**Optimization Strategies:**
- âœ… 5-minute cache TTL reduces 70% of classification calls
- âœ… Heuristic pre-filter (< 15 chars) ~10% speedup
- âœ… 5-second timeout prevents hanging requests
- âœ… Fail-safe: errors â†’ default to fast mode

---

### Configuration

**Environment Variables:**

```env
# Fast mode model (lightweight, quick responses)
AI_TEXT_MODEL=llama-3.1-8b-instant

# Thinking mode model (powerful, detailed responses)
AI_TOOL_MODEL=llama-3.3-70b-versatile
```

**Mode Customization:**

Edit `src/modules/chat/modes/mode.config.ts`:

```typescript
export const MODE_CONFIG: Record<EffectiveMode, ModeConfig> = {
  fast: {
    model: process.env.AI_TEXT_MODEL || 'llama-3.1-8b-instant',
    maxTokens: 500,        // Adjust max response length
    temperature: 0.5,      // Adjust creativity (0.0 - 1.0)
    systemPrompt: '...'    // Customize behavior
  },
  thinking: {
    model: process.env.AI_TOOL_MODEL || 'llama-3.3-70b-versatile',
    maxTokens: 4000,
    temperature: 0.7,
    systemPrompt: '...'
  }
};
```

---

### Benefits

**For Users:**
- ğŸš€ **Faster responses** for simple queries (fast mode)
- ğŸ§  **Deeper answers** for complex questions (thinking mode)
- ğŸ¤– **Automatic optimization** with auto mode (default)
- ğŸ¯ **Manual control** via mode override or conversation settings

**For System:**
- ğŸ’° **Cost optimization** - Use lightweight models when appropriate
- âš¡ **Performance** - Faster responses with smaller models
- ğŸ“Š **Observability** - Mode metadata tracked per message
- ğŸ”§ **Flexibility** - Easy to add new modes or adjust configs

---

## ğŸ”§ Tool System

### Architecture

The tool system is designed to be extensible and type-safe:

1. **Tool Interface** - Base contract for all tools
2. **Tool Registry** - Central registry for managing tools
3. **Tool Config** - Auto-registers tools on startup
4. **Tool Implementations** - Specific tool logic

### Creating a New Tool

```typescript
// 1. Create tool implementation
import { Injectable } from '@nestjs/common';
import { z } from 'zod';
import { Tool } from '../interfaces/tool.interface';

@Injectable()
export class CalculatorTool implements Tool {
  readonly name = 'calculator';
  
  readonly description = 'Performs basic arithmetic operations';
  
  readonly parameters = z.object({
    operation: z.enum(['add', 'subtract', 'multiply', 'divide']),
    a: z.number(),
    b: z.number(),
  });

  async execute(params: z.infer<typeof this.parameters>) {
    const { operation, a, b } = params;
    
    switch (operation) {
      case 'add': return a + b;
      case 'subtract': return a - b;
      case 'multiply': return a * b;
      case 'divide': return a / b;
    }
  }
}

// 2. Register in chat.module.ts
providers: [
  // ... existing providers
  CalculatorTool,
]

// 3. Register in tools.config.ts
onModuleInit() {
  this.toolRegistry.register(this.calculatorTool);
}
```

### Available Tools

#### Web Search Tool (`tavily_web_search`)

Searches the web for current information using Tavily API.

**Parameters:**
- `query` (string): Search query
- `maxResults` (number, optional): Max results (1-10, default: 5)

**Returns:**
```typescript
{
  query: string;
  results: Array<{
    title: string;
    url: string;
    snippet: string;
    favicon: string;
    publishedDate?: string;
    relevanceScore: number;
  }>;
  resultsCount: number;
  searchedAt: string;
  summary: string;
  citations: Array<{
    text: string;
    sourceIndex: number;
    url: string;
  }>;
}
```

---

## ğŸš¢ Deployment

This API is deployed on **DigitalOcean** using a production-grade architecture with Docker, PostgreSQL, Nginx reverse proxy, SSL certificates, and automated CI/CD.

**Live API**: `https://api.betterdev.in`

---

### Production Architecture

```
GitHub (push to main)
       â†“ CI/CD
GitHub Actions â”€â”€â”€> SSH into VPS â”€â”€â”€> Restart Docker Container
                                     (auto-build + health check)
DigitalOcean VPS (Ubuntu 22.04)
       â†“
NGINX (SSL, reverse proxy)
       â†“
Docker Container (NestJS API)
       â†“
PostgreSQL (VPS service)
```

---

### ğŸŒ DigitalOcean VPS Deployment

#### **Step 1: Prepare the VPS**

1. **Create a DigitalOcean Droplet** with Ubuntu 22.04 LTS
2. **SSH into the server**:
   ```bash
   ssh root@your-server-ip
   ```

3. **Create a non-root user** (best practice):
   ```bash
   adduser kashif
   usermod -aG sudo kashif
   su - kashif
   ```

---

#### **Step 2: Install Docker & Docker Compose**

```bash
# Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Add user to docker group (no sudo needed)
sudo usermod -aG docker $USER
newgrp docker

# Install Docker Compose
sudo apt-get update
sudo apt-get install docker-compose-plugin

# Verify installation
docker --version
docker compose version
```

---

#### **Step 3: Install PostgreSQL on VPS**

Instead of running PostgreSQL in Docker, we use a standalone database for stability and performance:

```bash
# Install PostgreSQL 14
sudo apt update
sudo apt install postgresql postgresql-contrib

# Create database and user
sudo -u postgres psql

CREATE DATABASE better_dev_db;
CREATE USER better_dev WITH PASSWORD 'your_secure_password';
GRANT ALL PRIVILEGES ON DATABASE better_dev_db TO better_dev;
\q

# Configure PostgreSQL to accept connections
sudo nano /etc/postgresql/14/main/postgresql.conf
# Set: listen_addresses = '*'

sudo nano /etc/postgresql/14/main/pg_hba.conf
# Add: host all all 0.0.0.0/0 md5

sudo systemctl restart postgresql

# Open firewall (if using UFW)
sudo ufw allow 5432/tcp
```

---

#### **Step 4: Clone Repository**

```bash
cd ~
git clone https://github.com/Kashif-Rezwi/better-dev-api.git
cd better-dev-api
```

---

#### **Step 5: Configure Environment**

Create `.env` file:

```bash
nano .env
```

Add environment variables:

```env
# Database
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_USER=better_dev
DATABASE_PASSWORD=your_secure_password
DATABASE_NAME=better_dev_db

# JWT
JWT_SECRET=your-super-secret-jwt-key-change-this
JWT_EXPIRATION=7d

# App
PORT=3001
NODE_ENV=production

# AI
GROQ_API_KEY=gsk_your_groq_api_key
DEFAULT_AI_MODEL=openai/gpt-oss-120b
AI_TEXT_MODEL=llama-3.1-8b-instant
AI_TOOL_MODEL=llama-3.3-70b-versatile

# Tools
TAVILY_API_KEY=tvly-your-tavily-api-key
```

---

#### **Step 6: Build & Start Docker Container**

```bash
# Build Docker image
docker compose build --no-cache

# Start container in detached mode
docker compose up -d

# Verify API is running
curl http://localhost:3001/health

# View logs
docker compose logs -f
```

---

#### **Step 7: Install & Configure Nginx Reverse Proxy**

```bash
# Install Nginx
sudo apt update
sudo apt install nginx

# Create Nginx configuration
sudo nano /etc/nginx/sites-available/api.betterdev.in
```

Add the following configuration:

```nginx
server {
    listen 80;
    server_name api.betterdev.in;

    location / {
        proxy_pass http://localhost:3001;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Enable the site:

```bash
# Create symbolic link
sudo ln -sf /etc/nginx/sites-available/api.betterdev.in /etc/nginx/sites-enabled/

# Test configuration
sudo nginx -t

# Reload Nginx
sudo systemctl reload nginx
```

---

#### **Step 8: Enable HTTPS with Certbot (Free SSL)**

```bash
# Install Certbot
sudo apt install certbot python3-certbot-nginx

# Obtain SSL certificate (auto-configures Nginx)
sudo certbot --nginx -d api.betterdev.in

# Verify auto-renewal
sudo certbot renew --dry-run
```

Now your API is accessible at: **`https://api.betterdev.in`**

---

#### **Step 9: Set Up GitHub Actions CI/CD (Auto Deploy)**

This enables automatic deployment on every push to `main` branch.

**On the VPS:**

1. **Generate SSH key for deployments** (no passphrase):
   ```bash
   ssh-keygen -t ed25519 -C "github-actions-deploy" -f ~/.ssh/github_actions_deploy
   
   # Add public key to authorized_keys
   cat ~/.ssh/github_actions_deploy.pub >> ~/.ssh/authorized_keys
   
   # Copy private key (you'll add this to GitHub Secrets)
   cat ~/.ssh/github_actions_deploy
   ```

**On GitHub:**

2. **Add SSH private key to GitHub Secrets**:
   - Go to your repo â†’ Settings â†’ Secrets and variables â†’ Actions
   - Add secret: `SSH_PRIVATE_KEY` (paste the private key content)
   - Add secret: `SERVER_IP` (your VPS IP address)
   - Add secret: `SERVER_USER` (your username, e.g., `kashif`)

3. **Create GitHub Actions workflow**:

Create `.github/workflows/deploy.yml`:

```yaml
name: Deploy to DigitalOcean

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    steps:
      - name: Deploy to VPS
        uses: appleboy/ssh-action@v1.0.0
        with:
          host: ${{ secrets.SERVER_IP }}
          username: ${{ secrets.SERVER_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            cd ~/better-dev-api
            git pull origin main
            docker compose down
            docker compose up -d --build
            docker compose logs --tail=50
```

Now, every push to `main` automatically deploys to production! ğŸš€

---

### ğŸ¯ Production Features

âœ… **Auto Deploy** - Push to GitHub â†’ automatic deployment  
âœ… **Auto Restart** - Docker restarts API if it crashes  
âœ… **Health Checks** - Docker monitors API health  
âœ… **SSL Auto-Renew** - Free HTTPS with auto-renewal  
âœ… **Isolated Database** - PostgreSQL runs independently  
âœ… **Reverse Proxy** - Nginx handles all traffic  
âœ… **Production-Grade** - Enterprise-level architecture  

---

### ğŸ”„ Managing Deployment

```bash
# SSH into VPS
ssh kashif@your-server-ip

# View logs
cd ~/better-dev-api
docker compose logs -f

# Restart API
docker compose restart

# Rebuild and restart
docker compose down
docker compose up -d --build

# Check health
curl https://api.betterdev.in/health

# View Nginx logs
sudo tail -f /var/log/nginx/access.log
sudo tail -f /var/log/nginx/error.log

# Check SSL certificate
sudo certbot certificates
```

---

### ğŸ³ Local Docker Development

For local development with Docker:

```bash
# Build and start all services
npm run docker:up

# View logs
npm run docker:logs

# Stop services
npm run docker:down

# Restart app only
npm run docker:restart
```

---

## ğŸ’» Development

### Available Scripts

```bash
# Development
npm run start:dev        # Start with hot-reload
npm run start:debug      # Start with debugger

# Production
npm run build            # Build for production
npm run start:prod       # Run production build

# Docker
npm run docker:build     # Build Docker images
npm run docker:up        # Start Docker containers
npm run docker:down      # Stop Docker containers
npm run docker:logs      # View logs
npm run docker:restart   # Restart app container

# Testing
npm run test             # Run tests
npm run test:watch       # Run tests in watch mode
npm run test:cov         # Generate coverage report
```

### Code Quality

```bash
# Linting
npx eslint .

# Format code
npx prettier --write "src/**/*.ts"
```

### Database Migrations

```bash
# Generate migration
npx typeorm migration:generate -n MigrationName

# Run migrations
npx typeorm migration:run

# Revert migration
npx typeorm migration:revert
```

---

## ğŸ” Environment Variables

### Required Variables

| Variable | Description | Example |
|----------|-------------|---------|
| `DATABASE_HOST` | PostgreSQL host | `localhost` |
| `DATABASE_PORT` | PostgreSQL port | `5432` |
| `DATABASE_USER` | Database user | `nebula_ai` |
| `DATABASE_PASSWORD` | Database password | `securepassword` |
| `DATABASE_NAME` | Database name | `nebula_db` |
| `JWT_SECRET` | JWT signing secret | `your-secret-key` |
| `JWT_EXPIRATION` | Token expiration | `7d` |
| `GROQ_API_KEY` | Groq API key | `gsk_...` |
| `TAVILY_API_KEY` | Tavily API key | `tvly-...` |

### Optional Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `PORT` | Server port | `3001` |
| `NODE_ENV` | Environment | `development` |
| `DEFAULT_AI_MODEL` | Default AI model | `openai/gpt-oss-120b` |
| `AI_TEXT_MODEL` | Fast text model | `llama-3.1-8b-instant` |
| `AI_TOOL_MODEL` | Tool calling model | `llama-3.3-70b-versatile` |

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Coding Standards

- Follow TypeScript best practices
- Use meaningful variable/function names
- Write self-documenting code
- Add comments for complex logic
- Maintain consistent formatting (use Prettier)
- Write unit tests for new features
- **Performance**: Use O(1) Map Lookups for data enrichment (avoid O(n) finds).
- **Optimization**: Use Column-Selective Queries to reduce DB I/O.
- **Memory**: Use memory-safe mapping instead of deep-cloning large history objects.

---

## ğŸ“„ License

This project is licensed under the UNLICENSED License.

---

## ğŸ™ Acknowledgments

- [NestJS](https://nestjs.com/) - Framework
- [Vercel AI SDK](https://sdk.vercel.ai/) - AI streaming
- [Groq](https://groq.com/) - Fast AI inference
- [Tavily](https://tavily.com/) - Web search API
- [TypeORM](https://typeorm.io/) - Database ORM

---

## ğŸ“ Support

For questions or issues:
- Open an [Issue](https://github.com/Kashif-Rezwi/better-dev-api/issues)
- Contact: [GitHub Profile](https://github.com/Kashif-Rezwi)

---

**Built with â¤ï¸ using NestJS and TypeScript**
